# Weekly Kaggle News #72
https://www.getrevue.co/profile/upura/issues/weekly-kaggle-news-72-574966
<h3><h2>News</h2><p>機械学習ライブラリ「<a href="https://github.com/scikit-learn/scikit-learn" target="_blank">scikit-learn</a>」内の交差検証用のクラスとして、新たに<a href="https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html" target="_blank">StratifiedGroupKFold</a>が実装されました。<a href="https://github.com/scikit-learn/scikit-learn/pull/18649" target="_blank">3月20日にmainブランチに取り込まれました</a>が、正式リリースについては未定のようです。Kaggle「<a href="https://www.kaggle.com/c/petfinder-adoption-prediction" target="_blank">PetFinder.my Adoption Prediction</a>」コンペで公開された<a href="https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation" target="_blank">Notebook</a>を参考に実装されています。</p><h2>Competitions</h2><p>認知症テストの結果を用いて、患者のアルツハイマー病の進行段階を分類する「<a href="https://www.alzheimersdata.org/funding-opportunities/data-science-challenge" target="_blank">ADDI Alzheimer’s Detection Challenge</a>」が始まりました。上位4チームへの賞金に加え「PlayStation 5」や「Xbox Series X」などの賞品も用意されています。</p><p>国際会議「SIGIR 2021」のEコマース系ワークショップ内で「<a href="https://sigir-ecom.github.io/data-task.html" target="_blank">In-session prediction for purchase intent and recommendations</a>」が21日に始まりました。購入アイテム予測・購買意図予測などが課題として設定されています。</p></h3>
<hr>
<p>
<img width="140" height="140" alt="Transformerは何をやっているのか - Speaker Deck" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/008/998/829/thumb/slide_0.jpg?1619455267" />
<strong style='display: block;'><a href="https://speakerdeck.com/yusuketakagi/transformerhahe-woyatuteirufalseka?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Transformerは何をやっているのか - Speaker Deck</a> &mdash; <a href="https://speakerdeck.com/yusuketakagi/transformerhahe-woyatuteirufalseka">speakerdeck.com</a></strong>
Transformerが結局のところ何をやっているのかを図メインで説明しています。
研究室の論文読み会で用いた資料の一部です。
</p>
<div style='clear: both;'></div>
<p><p>自然言語処理領域を中心に成果が報告されている「Transformer」の解説資料。データの流れなどを図解中心で説明しています。</p></p>
<p>
<img width="140" height="140" alt="AutoNLPを使った日本語文書分類 | 株式会社AI Shift" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/008/998/831/thumb/44c23b6b15d70994d766716b66bcaf1c.png?1619455287" />
<strong style='display: block;'><a href="https://www.ai-shift.co.jp/techblog/1880?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">AutoNLPを使った日本語文書分類 | 株式会社AI Shift</a> &mdash; <a href="https://www.ai-shift.co.jp/techblog/1880">www.ai-shift.co.jp</a></strong>
AI ShiftのTECH BLOGです。AI技術の情報や活用方法などをご案内いたします。
</p>
<div style='clear: both;'></div>
<p><p><a href="https://twitter.com/abhi1thakur/status/1384875565761679360?s=20" target="_blank">21日に日本語対応</a>したライブラリ「AutoNLP」を用いて文書分類に取り組んだ記事。使い方を示すだけではなく、線形回帰やBERTとも性能を比較しています。</p></p>
<p>
<img width="140" height="140" alt="社内CV輪講資料 / PyTorch Lightning - Speaker Deck" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/008/999/435/thumb/slide_0.jpg?1619458064" />
<strong style='display: block;'><a href="https://speakerdeck.com/takarasawa_/pytorch-lightning?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">社内CV輪講資料 / PyTorch Lightning - Speaker Deck</a> &mdash; <a href="https://speakerdeck.com/takarasawa_/pytorch-lightning">speakerdeck.com</a></strong>
<p>社内のCV輪講で PyTorch Lightningについて共有したときの資料です</p>
</p>
<div style='clear: both;'></div>
<p><p>深層学習ライブラリ「PyTorch」の学習コードを簡略化できる「PyTorch Lightning」の紹介記事。基本的な理念や、具体的に実現できる機能が端的にまとめられています。</p></p>
<p>
<img width="140" height="140" alt="Release v0.3.0 Notebook launcher and multi-node training · huggingface/accelerate · GitHub" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/009/053/444/thumb/accelerate?1619723902" />
<strong style='display: block;'><a href="https://github.com/huggingface/accelerate/releases/tag/v0.3.0?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Release v0.3.0 Notebook launcher and multi-node training · huggingface/accelerate · GitHub</a> &mdash; <a href="https://github.com/huggingface/accelerate/releases/tag/v0.3.0">github.com</a></strong>
A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision - huggingface/accelerate
</p>
<div style='clear: both;'></div>
<p><p>PyTorchのコードにおけるCPUとGPUなどのデバイス間のやり取りを処理するライブラリ「<a href="https://huggingface.co/blog/accelerate-library?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank">Accelerate</a>」のv0.3.0が30日に公開されました。TPUを利用する際に便利な関数や、<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/accelerate/simple_nlp_example.ipynb" target="_blank">Notebookの例</a>などが追加されています。</p></p>
<p>
<img width="140" height="140" alt="Focal Lossによる自信過剰な予測の抑制 - MicroAd Developers Blog" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/009/005/002/thumb/20210402165251.png?1619492695" />
<strong style='display: block;'><a href="https://developers.microad.co.jp/entry/2021/04/26/060000?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Focal Lossによる自信過剰な予測の抑制 - MicroAd Developers Blog</a> &mdash; <a href="https://developers.microad.co.jp/entry/2021/04/26/060000">developers.microad.co.jp</a></strong>
<p>今回はCTR/CVR予測の学習にFocal Loss [Tsung-Yi Lin et al., 2017]と呼ばれる損失関数を使ってみたのでその結果を紹介したいと思います.</p>
</p>
<div style='clear: both;'></div>
<p><p>LightGBMの損失関数として、不均衡データに有効とされる「Focal Loss」を用いた記事。数式・実装・実験結果が掲載されています。</p></p>
<p>
<img width="140" height="140" alt="PyTorchのPERFORMANCE TUNING GUIDEの効果を確認してみる その1 「parameter.grad = Noneを使う」 - まったり勉強ノート" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/009/030/053/thumb/tb_overview-150x150.png?1619597436" />
<strong style='display: block;'><a href="https://www.mattari-benkyo-note.com/2021/04/27/pytorch-performance-tuning-guide-1-parameter-grad-none/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">PyTorchのPERFORMANCE TUNING GUIDEの効果を確認してみる その1 「parameter.grad = Noneを使う」 - まったり勉強ノート</a> &mdash; <a href="https://www.mattari-benkyo-note.com/2021/04/27/pytorch-performance-tuning-guide-1-parameter-grad-none/">www.mattari-benkyo-note.com</a></strong>
<p>PyTorchには「PERFORMANCE TUNING GUIDE」という学習を速くするためのテクニック集があります。</p>
</p>
<div style='clear: both;'></div>
<p><p>PyTorchの公式ドキュメントとして公開されている「<a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html" target="_blank">Performance Tuning Guide</a>」に記載の技法を検証する記事。第1弾として&nbsp;「<a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad" target="_blank">Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()</a>」の項目について、具体的な検証結果を掲載しています。</p></p>
<p>
<img width="140" height="140" alt="Big Birdの紹介 - Retrieva TECH BLOG" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/009/035/643/thumb/159479255596133?1619620465" />
<strong style='display: block;'><a href="https://tech.retrieva.jp/entry/2021/04/28/172553?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Big Birdの紹介 - Retrieva TECH BLOG</a> &mdash; <a href="https://tech.retrieva.jp/entry/2021/04/28/172553">tech.retrieva.jp</a></strong>
昨年のNeurIPSで発表されたBig Birdという手法の紹介です。
</p>
<div style='clear: both;'></div>
<p><p>Transformerモデルの一種として発表された「<a href="https://papers.nips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html" target="_blank">Big Bird</a>」の紹介記事。長文を扱えるよう、注意機構に関する工夫が施されています。</p></p>
<p>
<strong style='display: block;'><a href="https://copernicus-masters-japan.digitalblast.co.jp/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">COPERVIXUS PRIZE JAPAN</a></strong>
<p>複数の課題の中から1つのみを選択し、その課題を解決するビジネスアイディアを1つ考えてください。ビジネスアイディアは、2つの部門で評価されます。</p>
</p>
<p><p>衛星データを活用したビジネスアイディアコンテスト「コペルニクス・マスターズ」の日本大会が28日に始まりました。情報交換の場として<a href="https://www.nishika.com/competitions/15/summary" target="_blank">Nishikaのプラットフォーム</a>が活用されています。</p></p>
