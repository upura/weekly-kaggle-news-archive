# Weekly Kaggle News #6
https://www.getrevue.co/profile/upura/issues/weekly-kaggle-news-6-221555
<h3><h2>News</h2><p>1位チームの賞金$100,000で話題となった「<a href="https://www.kaggle.com/c/data-science-bowl-2019" target="_blank">2019 Data Science Bowl</a>」コンペなど、22〜23日に3つのKaggleコンペが終了しました。結果に一喜一憂するTwitter投稿で大いに盛り上がった週でした。</p><h2>Competitions</h2><p>単一画像から車両の姿勢を推定する「<a href="https://www.kaggle.com/c/pku-autonomous-driving" target="_blank">Peking University/Baidu - Autonomous Driving</a>」コンペが22日に終了しました。<a href="https://www.kaggle.com/c/pku-autonomous-driving/discussion/117578" target="_blank">序盤のLB表示の不具合</a>や<a href="https://www.kaggle.com/c/pku-autonomous-driving/discussion/127060" target="_blank">テストデータ内のダミー画像</a>など、運営に関する部分も目立ってしまったコンペだったように感じます。「<a href="https://speakerdeck.com/ryosukehata/monocular-3d-object-detection-survey" target="_blank">Monocular 3D Object Detection Survey</a>」は、コンペの題材となった学術領域に関して日本語でまとめられているスライドです。</p><p>翌23日には、子供向けゲームのアクセスログを分析する「2019 Data Science Bowl」コンペと、質疑応答が題材の「<a href="https://www.kaggle.com/c/tensorflow2-question-answering" target="_blank">TensorFlow 2.0 Question Answering</a>」コンペが終わりました。前者は「<a href="https://www.kaggle.com/c/data-science-bowl-2019/overview/evaluation" target="_blank">QWK</a>」というブレやすい評価指標を採用しており、大きめのshake upが発生しました。データセットから特徴量を作る形式が若干独特でしたが、アクセスログの特徴量エンジニアリングなど他コンペや業務で参考にできる場面も多そうです。</p><p>後者は、Kaggleでは出題頻度が少ない自然言語処理の質疑応答がタスクのコンペでした。<a href="https://www.nikkei.com/article/DGXMZO51391220V21C19A0TJC000/" target="_blank">Googleが検索エンジンにBERTを利用する</a>など、近年大きな進歩を遂げている領域です。TensorFlow 2.0のリリースを受けて、<a href="https://www.kaggle.com/c/tensorflow2-question-answering/overview/prizes" target="_blank">TensorFlow 2.0を使ったチーム向けの賞も設定されていました</a>。</p></h3>
<hr>
<p>
<img width="140" height="140" alt="Optuna の拡張機能 LightGBM Tuner によるハイパーパラメータ自動最適化 | Preferred Networks Research &amp; Development" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/447/339/thumb/image5.png?1579491153" />
<strong style='display: block;'><a href="https://tech.preferred.jp/ja/blog/hyperparameter-tuning-with-optuna-integration-lightgbm-tuner/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Optuna の拡張機能 LightGBM Tuner によるハイパーパラメータ自動最適化 | Preferred Networks Research &amp; Development</a> &mdash; <a href="https://tech.preferred.jp/ja/blog/hyperparameter-tuning-with-optuna-integration-lightgbm-tuner/">tech.preferred.jp</a></strong>
Optuna 開発メンバの小嵜 (@smly) です。この記事では Optuna の拡張機能として開発している LightGBM Tuner について紹介します。 LightGBM Tuner は LightGBM に特化したハイパーパラメータ自動最適化のためのモジュールです。Pyhton コードの
</p>
<div style='clear: both;'></div>
<p><p>ハイパーパラメータ調整ツール「Optuna」の拡張機能として開発されている「LightGBM Tuner」の紹介記事。「重要なハイパーパラメータから順に最適なハイパーパラメータをチューニングする」というKagglerの経験則に基づいて実装されています。</p></p>
<p>
<img width="140" height="140" alt="Kaggler Interviews &amp; Highlights" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/437/690/thumb/1*9izrRVNdAJa9bFaqBwSH4w.png?1579233072" />
<strong style='display: block;'><a href="https://medium.com/kaggle-blog?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Kaggler Interviews &amp; Highlights</a> &mdash; <a href="https://medium.com/kaggle-blog">medium.com</a></strong>
Official Kaggle Blog ft. interviews from top data science competitors and more!
</p>
<div style='clear: both;'></div>
<p><p>Kaggleの公式ブログがmediumに移行されました。上位入賞したKagglerへのインタビューなどが掲載されており、読み応えがあります。</p></p>
<p>
<strong style='display: block;'><a href="https://arxiv.org/abs/2001.05166?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">[2001.05166] ShapeVis: High-dimensional Data Visualization at Scale</a></strong>

</p>
<p><p>トポロジカルデータ分析から着想を得た大規模な高次元データの可視化手法「ShapeVis」の提案。t-SNE、UMAPなどと比較し、データ可視化の品質やデータ量を増やした際のスケーラビリティを議論しています。</p></p>
<p>
<img width="140" height="140" alt="SHapley Additive exPlanationsで機械学習モデルを解釈する - Speaker Deck" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/437/691/thumb/slide_0.jpg?1579233089" />
<strong style='display: block;'><a href="https://speakerdeck.com/dropout009/shapley-additive-explanationsdeji-jie-xue-xi-moderuwojie-shi-suru?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">SHapley Additive exPlanationsで機械学習モデルを解釈する - Speaker Deck</a> &mdash; <a href="https://speakerdeck.com/dropout009/shapley-additive-explanationsdeji-jie-xue-xi-moderuwojie-shi-suru">speakerdeck.com</a></strong>

</p>
<div style='clear: both;'></div>
<p><p>「<a href="https://upura.hatenablog.com/entry/2020/01/16/205914" target="_blank">Data Gateway Talk vol.5</a>」というイベントで発表された、機械学習モデルの解釈手法「SHAP」の解説資料。単なるライブラリの使い方にとどまらず、理論的な背景から丁寧に説明しています。</p></p>
<p>
<img width="140" height="140" alt="私たちはいかにして環状線で”悪さをする列車”を捕まえたか | POSTD" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/437/704/thumb/postd_logo_2x.png?1579233783" />
<strong style='display: block;'><a href="https://postd.cc/how-we-caught-the-circle-line-rogue-train-with-data/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">私たちはいかにして環状線で”悪さをする列車”を捕まえたか | POSTD</a> &mdash; <a href="https://postd.cc/how-we-caught-the-circle-line-rogue-train-with-data/">postd.cc</a></strong>
*文：Daniel Sim　分析：Lee Shangqian、Daniel Sim、Clarence
</p>
<div style='clear: both;'></div>
<p><p>シンガポールの列車がたびたび緊急停車する事象の原因をデータ分析で特定した話。Jupyter Notebook上で可視化しながら仮説検証を繰り返し原因を絞り込んでいく過程は、Kaggleの取り組みに類似している面を感じました。</p></p>
<p>
<img width="140" height="140" alt="日本語BERTモデルをPyTorch用に変換してfine-tuningする with torchtext &amp; pytorch-lightning - radiology-nlp’s blog" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/442/128/thumb/20191212215558.png?1579302255" />
<strong style='display: block;'><a href="https://radiology-nlp.hatenablog.com/entry/2020/01/18/013039?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">日本語BERTモデルをPyTorch用に変換してfine-tuningする with torchtext &amp; pytorch-lightning - radiology-nlp’s blog</a> &mdash; <a href="https://radiology-nlp.hatenablog.com/entry/2020/01/18/013039">radiology-nlp.hatenablog.com</a></strong>
TL;DR ①TensorFlow版訓練済みモデルをPyTorch用に変換した (→方法だけ読みたい方はこちら) ②①をスムーズに使うための torchtext.data.Dataset を設計した ③PyTorch-Lightningを使ってコードを短くした はじめに 日本語Wikipediaで事前学習されたBERTモデルとしては, 以下の2つが有名であり, 広く普及しています: SentencePieceベースのモデル (Yohei Kikuta さん提供) TensorFlow版 Juman++ベースのモデル (京大黒橋研提供) TensorFlow版 PyTorch版(Hugging …
</p>
<div style='clear: both;'></div>
<p><p>日本語Wikipediaで事前学習されたSentencePieceベースのBERTモデルをPyTorchでfine tuningするためのチュートリアル記事。各ステップでの処理が、コードを含めて丁寧に記載されています。</p></p>
<p>
<img width="140" height="140" alt="MixedPrecisionのすゝめ" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/447/148/thumb/mixedprecision-200119040437-thumbnail-4.jpg?1579483991" />
<strong style='display: block;'><a href="https://www.slideshare.net/ssuser21af5b/mixedprecision?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">MixedPrecisionのすゝめ</a> &mdash; <a href="https://www.slideshare.net/ssuser21af5b/mixedprecision">www.slideshare.net</a></strong>
Mixed Precisionのすゝめ 第56回 コンピュータビジョン勉強会＠関東での発表資料
</p>
<div style='clear: both;'></div>
<p><p>精度を維持しつつメモリ軽減や計算高速化を実現する「Mixed Precision」の解説スライド。PyTorchを用いた実験例や、Kaggle Masterである発表者の知見が盛り込まれています。</p></p>
<p>
<strong style='display: block;'><a href="https://neongen-ai.github.io/2020/01/18/how_I_found_my_current_job.html?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">How I found my current job | Konstantinos Fanopoulos</a></strong>
As a new decade has begun, I feel the need to look back a little at how I entered the industry and launched a career as Data Scientist / ML Engineer as well as to set my plans for the future.
</p>
<p><p><a href="https://www.kaggle.com/neongen" target="_blank">Kaggle過去最高ランク38位の方</a>が、Kaggleと共に歩んだ現在までの経歴に関する記事。初めての転職を期にKaggleに注力し始めた理由などを紹介しています。</p></p>
<p>
<img width="140" height="140" alt="Page not found - Open Source Leader in AI and ML" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/464/231/thumb/SRK-MLP-H2O-World-NYC-1024x683.jpg?1579830369" />
<strong style='display: block;'><a href="https://www.h2o.ai/blog/how-the-passion-for-numbers-turned-this-mechanical-engineer-into-a-kaggle-grandmaster/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Page not found - Open Source Leader in AI and ML</a> &mdash; <a href="https://www.h2o.ai/blog/how-the-passion-for-numbers-turned-this-mechanical-engineer-into-a-kaggle-grandmaster/">www.h2o.ai</a></strong>

</p>
<div style='clear: both;'></div>
<p><p><a href="https://www.kaggle.com/sudalairajkumar" target="_blank">Kaggle GrandmasterのSRKさん</a>に関する同様の記事も公開されました。これまでの経歴や、データサイエンティストへのメッセージが掲載されています。</p></p>
<p>
<strong style='display: block;'><a href="https://www.youtube.com/watch?feature=youtu.be&amp;utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter&amp;v=0AdaTRU--YE">Kaggle Santa 2019で学ぶMIP最適化入門</a></strong>
Kaggle santa2019コンペの解法解説とCPLEX, GurobiとPuLP＋CBCでの解き方をライブコーディングする動画 解法の解説だけ見たい方は#19:03から参照ください。 Github: https://github.com/tkm2261/kaggle_santa2019_youtube Sl...
</p>
<p><p>先日終了したKaggle「<a href="https://www.kaggle.com/c/santa-workshop-tour-2019?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank">Santa’s Workshop Tour 2019</a>」コンペで最適解を出したKaggle Masterの方による解説動画。約90分もの長尺で、コンペ解法だけではなく、数理最適化全体も概説しています。</p></p>
<p>
<strong style='display: block;'><a href="https://www.youtube.com/watch?feature=youtu.be&amp;utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter&amp;v=g5DSLeJozdw">Deep Learning入門：Attention（注意）</a></strong>
Deep LearningにおいてConvolutional Neural Networksに並んで大変ポピュラーに用いられつつあるニューラルネットワークの基本的な構造、Attention（注意）について解説します。 前回の動画：「量子化によるニューラルネットワークのコンパクト化」 https://www.you...
</p>
<p><p>ニューラルネットワークの基本的な構造「Attention」の解説動画。この<a href="https://www.youtube.com/channel/UCRTV5p4JsXV3YTdYpTJECRA" target="_blank">チャンネル</a>では、Deep learningの分かりやすい動画が定期的に投稿されています。</p></p>
