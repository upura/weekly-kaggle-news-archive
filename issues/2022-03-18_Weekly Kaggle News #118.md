# Weekly Kaggle News #118
https://www.getrevue.co/profile/upura/issues/weekly-kaggle-news-118-1079646
<h3><h2>News &amp; Competitions</h2><p>Kaggle「<a href="https://www.kaggle.com/c/feedback-prize-2021" target="_blank">Feedback Prize - Evaluating Student Writing</a>」が15日に終わりました。学生が書いた議論用の文章を解析する自然言語処理コンペでした。</p><p>国際学会「<a href="https://kdd.org/kdd2022/" target="_blank">KDD 2022</a>」で「KDD Cup」として開催される2つのコンペ情報が公開されました。AmazonのECサイトのデータセットを用いる「<a href="https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search" target="_blank">Amazon Product Search</a>」と、風力発電予測が題材の「<a href="https://aistudio.baidu.com/aistudio/competition/detail/152/0/introduction" target="_blank">Wind Power Forecast</a>」が開催されます。</p></h3>
<hr>
<p>
<img width="140" height="140" alt="[2203.05482] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/014/677/823/thumb/arxiv-logo-twitter-square.png?1647231088" />
<strong style='display: block;'><a href="https://arxiv.org/abs/2203.05482?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">[2203.05482] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</a> &mdash; <a href="https://arxiv.org/abs/2203.05482">arxiv.org</a></strong>
<p>In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin.</p>
</p>
<div style='clear: both;'></div>
<p><p>モデルの出力ではなく重みを混ぜ合わせるアンサンブル手法を提案している論文。推論時のモデルが1つで済む利点があります。</p></p>
<p>
<img width="140" height="140" alt="GitHub - oreilly-japan/deep-learning-from-scratch-4" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/014/692/245/thumb/deep-learning-from-scratch-4?1647278083" />
<strong style='display: block;'><a href="https://github.com/oreilly-japan/deep-learning-from-scratch-4?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">GitHub - oreilly-japan/deep-learning-from-scratch-4</a> &mdash; <a href="https://github.com/oreilly-japan/deep-learning-from-scratch-4">github.com</a></strong>
Contribute to oreilly-japan/deep-learning-from-scratch-4 development by creating an account on GitHub.
</p>
<div style='clear: both;'></div>
<p><p>書籍『<a href="https://www.amazon.co.jp/dp/4873119758" target="_blank">ゼロから作るDeep Learning ❹ 強化学習編</a>』のサポートサイト。サンプルコードがKaggle上のNotebookなどの形式で公開されています。</p></p>
[embed https://youtu.be/pdUpeBip3Z8]
<p><p>「<a href="https://dena.connpass.com/event/237949/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank">DeNA TechCon 2022</a>」内のセッションとして開催されたパネルディスカッションの動画。独自の<a href="https://dena.ai/kaggle/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank">Kaggle社内ランク制度</a>を設けるDeNAに所属する4人のKaggle Grandmasterが登壇しました。</p></p>
