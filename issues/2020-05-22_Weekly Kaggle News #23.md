# Weekly Kaggle News #23
https://www.getrevue.co/profile/upura/issues/weekly-kaggle-news-23-248955
<h3><h2>News</h2><p>KaggleのNotebookに新機能「Load from URL」が追加されました。GitHubなどのURLを指定し、ファイルをインポートできます（<a href="https://twitter.com/MeganRisdal/status/1262952384487206914?s=20" target="_blank">デモ</a>）。Notebookからの予測結果提出方法のUIUXが<a href="https://youtu.be/ESMKzVsR7wE" target="_blank">一時的に変更</a>になるなど、日々改修が重ねられています。</p><h2>Competitions</h2><p>Googleが公開している大規模データセットを用いたコンペ「<a href="https://www.kaggle.com/c/open-images-object-detection-rvc-2020/overview" target="_blank">Open Images Object Detection RVC 2020 edition</a>」「<a href="https://www.kaggle.com/c/open-images-instance-segmentation-rvc-2020" target="_blank">Open Images Instance Segmentation RVC 2020 edition</a>」が19日に始まりました。今年は「<a href="https://storage.googleapis.com/openimages/web/challenge_overview.html" target="_blank">Robust Vision Challenge 2020</a>」という枠組みでの開催となり、Competitionのメダルは付与されません。</p><p>Nishika「<a href="https://www.nishika.com/competitions/4/summary" target="_blank">財務・非財務情報を活用した株主価値予測</a>」コンペが22日に終わりました。TIS株式会社が公開している財務・非財務情報から、期末時価総額を予測するタスクでした。</p><h2>PR</h2><p>『<a href="https://www.amazon.co.jp/dp/B088R992TJ" target="_blank">PythonではじめるKaggleスタートブック</a>』のKindle版の予約が始まりました。26日配信予定です。</p></h3>
<hr>
<p>
<img width="140" height="140" alt="kagglerを訪ねて三千里（リモート）smly (@smly)さん" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/006/004/462/thumb/maxresdefault_live.jpg?1590122871" />
<strong style='display: block;'><a href="https://www.youtube.com/watch?feature=youtu.be&amp;utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter&amp;v=BSvP60BzoOc">kagglerを訪ねて三千里（リモート）smly (@smly)さん</a> &mdash; <a href="https://www.youtube.com/watch?v=BSvP60BzoOc&amp;feature=youtu.be">www.youtube.com</a></strong>
ゲスト: smly (@smly)さん 質問はこちら https://docs.google.com/forms/d/e/1FAIpQLSeCHH9iJyjdeZAGhklBqODIyw-kNgsFE3kp4Noy7gd0zziqFA/viewform?usp=sf_link
</p>
<div style='clear: both;'></div>
<p><p>「kagglerを訪ねて三千里」と題した日本人Kagglerインタビュー企画の第3弾が23日に放送予定。ゲストはKaggle Grandmasterの<a href="https://www.kaggle.com/confirm" target="_blank">smly</a>さんで、<a href="https://t.co/TK2xMDzcg7?amp=1" target="_blank">質問</a>も募集されています。</p></p>
<p>
<img width="140" height="140" alt="「BERT応用勉強会」参加録 #xpaperchallenge - u++の備忘録" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/006/000/133/thumb/20200515191009.png?1590050882" />
<strong style='display: block;'><a href="https://upura.hatenablog.com/entry/2020/05/15/211833?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">「BERT応用勉強会」参加録 #xpaperchallenge - u++の備忘録</a> &mdash; <a href="https://upura.hatenablog.com/entry/2020/05/15/211833">upura.hatenablog.com</a></strong>
「BERT応用勉強会」にオンライン参加しました。簡単な発表概要と個人的な所感をメモしておきます。発表動画のアーカイブは、後日YouTubeで公開されるそうです。slidoとYouTubeコメントでの質疑応答はSpreadsheetにまとめてみました。 nlpaper-challenge.connpass.com 医療言語処理へのBERTの応用 --BioBERT, ClinicalBERT, そして-- 発表資料 概要 所感 Multilingual BERTの二言語領域適応に基づく対訳文同定 概要 所感 BERTのMulti Modalタスクへの活用 発表資料 概要 所感 BERTをブラウザ…
</p>
<div style='clear: both;'></div>
<p><p>15日開催の「BERT応用勉強会」の聴講メモ。BERTに関する興味深い発表5件でした。アーカイブ動画も公開されています。</p></p>
<p>
<strong style='display: block;'><a href="https://qiita.com/YuiKasuga/items/343309257da1798c1b63?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">BERTの精度を向上させる手法10選 - Qiita</a></strong>

</p>
<p><p>近年のNLPを語る上で欠かせないBERTについて、Kaggleコンペなどで登場した性能向上手法をまとめた記事。文を最大長の指定に基づき抽出する工夫や、層ごとの処理などが挙げられています。</p></p>
<p>
<img width="140" height="140" alt="Siamese and Dual BERT for Multi Text Classification" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/983/771/thumb/0*3m3xJSK38fsrOt6k?1589761939" />
<strong style='display: block;'><a href="https://towardsdatascience.com/siamese-and-dual-bert-for-multi-text-classification-c6552d435533?gi=e70f3ef32145&amp;utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Siamese and Dual BERT for Multi Text Classification</a> &mdash; <a href="https://towardsdatascience.com/siamese-and-dual-bert-for-multi-text-classification-c6552d435533?gi=e70f3ef32145">towardsdatascience.com</a></strong>
Constant research in NLP produces the development of various kinds of pre-trained models. It’s typical to register increasing improvements in state-of-the-art results for various tasks, such as text…
</p>
<div style='clear: both;'></div>
<p><p>Kaggle「<a href="https://www.kaggle.com/c/google-quest-challenge" target="_blank">Google QUEST Q&amp;A Labeling</a>」コンペの2nd place solutionのアイディアの検証記事。text1とtext2を結合してからBERTに入れるのではなく、それぞれ別のBERTに入れた後に結合する手法で分類性能が上がっています。</p></p>
<p>
<img width="140" height="140" alt="A Visual Survey of Data Augmentation in NLP" style="float: right; margin-left: 20px; margin-bottom: 20px;" src="https://s3.amazonaws.com/revue/items/images/005/984/319/thumb/semantic-invariance-nlp.png?1589780341" />
<strong style='display: block;'><a href="https://amitness.com/2020/05/data-augmentation-for-nlp/?utm_campaign=Weekly%20Kaggle%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">A Visual Survey of Data Augmentation in NLP</a> &mdash; <a href="https://amitness.com/2020/05/data-augmentation-for-nlp/">amitness.com</a></strong>
An extensive overview of text data augmentation techniques for Natural Language Processing
</p>
<div style='clear: both;'></div>
<p><p>NLPにおけるデータ水増し手法が図示されているまとめ記事。語彙変換・翻訳・表層・交叉など。arXivやGitHubやKaggleへのリンクが記載されているのも魅力的です。</p></p>
